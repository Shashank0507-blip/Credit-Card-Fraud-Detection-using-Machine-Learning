{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8249d7e0-ad35-4033-ac35-a13b3bd0e37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (284807, 31)\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n",
      "None\n",
      "[LightGBM] [Info] Number of positive: 227451, number of negative: 227451\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 454902, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9396784095217918, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9396784095217918\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.1506549622849023, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1506549622849023\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5531550244558602, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5531550244558602\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9396784095217918, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9396784095217918\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.1506549622849023, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1506549622849023\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5531550244558602, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5531550244558602\n",
      "[LightGBM] [Info] Number of positive: 227451, number of negative: 227451\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065899 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 454902, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Best Parameters (LightGBM): OrderedDict([('bagging_fraction', 0.5531550244558602), ('feature_fraction', 0.9396784095217918), ('lambda_l2', 1.1506549622849023), ('learning_rate', 0.2914997906774421), ('max_depth', 15), ('n_estimators', 492)])\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9396784095217918, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9396784095217918\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.1506549622849023, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1506549622849023\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5531550244558602, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5531550244558602\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9396784095217918, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9396784095217918\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.1506549622849023, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1506549622849023\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5531550244558602, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5531550244558602\n",
      "âœ… Selected Features (Mutual Information): ['V14', 'V17', 'V10', 'V12', 'V4', 'Amount', 'V11', 'V3', 'V16', 'V7', 'V2', 'V9', 'V27', 'V21', 'V18', 'V1', 'V6', 'V28', 'V5', 'V8']\n",
      "âœ… Selected Features(Overall): ['V14', 'V17', 'V4', 'Amount', 'V3', 'V9', 'V18', 'V1', 'V5', 'V8']\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9396784095217918, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9396784095217918\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.1506549622849023, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1506549622849023\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5531550244558602, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5531550244558602\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9396784095217918, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9396784095217918\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.1506549622849023, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1506549622849023\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5531550244558602, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5531550244558602\n",
      "--------------------------------------------------------------------------\n",
      "ðŸ“Š Model Accuracy Before Feature Selection: 0.9983\n",
      "ðŸ“Š Model Precision Before Feature Selection: 0.5000\n",
      "ðŸ“Š Model Recall Before Feature Selection: 0.8878\n",
      "ðŸ“Š Model F1 Score Before Feature Selection: 0.6397\n",
      "âœ… Notebook saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from skopt.space import Integer, Categorical\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from skopt.space import Real\n",
    "import dill\n",
    "\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "X = df.drop(columns=['Class'])\n",
    "y = df['Class']\n",
    "scaler = StandardScaler()\n",
    "X[['Time', 'Amount']] = scaler.fit_transform(X[['Time', 'Amount']])\n",
    "\n",
    "with open(\"scaler.pkl\", \"wb\") as file:\n",
    "    dill.dump(scaler, file)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "####Smote\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "###let's use a lightgbm algorithm here for now\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "lgbm.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "####Finding the best parameters\n",
    "param_space_lgbm = {\n",
    "    'n_estimators': Integer(50, 500),\n",
    "    'max_depth': Integer(3, 15),\n",
    "    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "    'lambda_l2': Real(1, 5),\n",
    "    'bagging_fraction': Real(0.5, 1.0),\n",
    "    'feature_fraction': Real(0.5, 1.0)\n",
    "}\n",
    "bayesian_search_lgbm = BayesSearchCV(lgbm, param_space_lgbm, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "bayesian_search_lgbm.fit(X_train_balanced, y_train_balanced)\n",
    "print(f\"Best Parameters (LightGBM): {bayesian_search_lgbm.best_params_}\")\n",
    "\n",
    "###Trian the model using the best features from above\n",
    "best_lgbm = LGBMClassifier(random_state=42, verbose=0, **bayesian_search_lgbm.best_params_)\n",
    "best_lgbm.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "#########Save the model\n",
    "import dill\n",
    "with open(\"fraudulent.pkl\", \"wb\") as file:\n",
    "    dill.dump(best_lgbm, file)\n",
    "\"\"\"\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "####using MI\n",
    "mi_scores = mutual_info_classif(X_train_balanced, y_train_balanced)\n",
    "mi_df = pd.DataFrame({'Feature': X_train_balanced.columns, 'MI Score': mi_scores})\n",
    "mi_df = mi_df.sort_values(by='MI Score', ascending=False)\n",
    "selected_features_mi = mi_df['Feature'].head(20).tolist()\n",
    "print(f\" Selected Features (Mutual Information): {selected_features_mi}\")\n",
    "################\n",
    "X_train_mi = X_train_balanced[selected_features_mi]\n",
    "rfe = RFE(estimator=lgbm, n_features_to_select=10)\n",
    "rfe.fit(X_train_mi, y_train_balanced)\n",
    "selected_features = X_train_mi.columns[rfe.support_].tolist()\n",
    "print(f\" Selected Features(Overall): {selected_features}\")\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#######Train the models based on the selected features\n",
    "X_train_selected = X_train_balanced[selected_features]  \n",
    "X_test_selected = X_test[selected_features]\n",
    "lgbm.fit(X_train_selected, y_train_balanced)\n",
    "y_pred = lgbm.predict(X_test_selected)\n",
    "\n",
    "#Formulate for diff measureing quantities\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "#Print out the scores after using feature selection\n",
    "print(f\" Model Accuracy After Feature Selection: {accuracy:.4f}\")\n",
    "print(f\" Model Precision After Feature Selection: {precision:.4f}\")\n",
    "print(f\" Model Recall After Feature Selection: {recall:.4f}\")\n",
    "print(f\" Model F1 Score After Feature Selection: {f1:.4f}\")\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "#without using feature selection and using all the features\n",
    "\n",
    "best_lgbm.fit(X_train_balanced, y_train_balanced)\n",
    "y_pred1 = lgbm.predict(X_test)\n",
    "\n",
    "#####\n",
    "#Formulate for diff measureing quantities\n",
    "accuracy1 = accuracy_score(y_test, y_pred1)\n",
    "precision1 = precision_score(y_test, y_pred1)\n",
    "recall1 = recall_score(y_test, y_pred1)\n",
    "f11 = f1_score(y_test, y_pred1)\n",
    "\n",
    "#Print out the scores before using feature selection\n",
    "print(f\"ðŸ“Š Model Accuracy Before Feature Selection: {accuracy1:.4f}\")\n",
    "print(f\"ðŸ“Š Model Precision Before Feature Selection: {precision1:.4f}\")\n",
    "print(f\"ðŸ“Š Model Recall Before Feature Selection: {recall1:.4f}\")\n",
    "print(f\"ðŸ“Š Model F1 Score Before Feature Selection: {f11:.4f}\")\n",
    "\n",
    "\n",
    "print(\"âœ… Notebook saved successfully!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882145d1-3c3e-4e4d-a54b-e27e9f68e615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
