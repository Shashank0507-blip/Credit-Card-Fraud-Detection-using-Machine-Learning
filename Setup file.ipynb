{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8249d7e0-ad35-4033-ac35-a13b3bd0e37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (284807, 31)\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n",
      "None\n",
      "[LightGBM] [Info] Number of positive: 227451, number of negative: 227451\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 454902, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9396784095217918, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9396784095217918\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.1506549622849023, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1506549622849023\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5531550244558602, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5531550244558602\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9396784095217918, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9396784095217918\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.1506549622849023, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1506549622849023\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5531550244558602, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5531550244558602\n",
      "[LightGBM] [Info] Number of positive: 227451, number of negative: 227451\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065899 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 454902, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Best Parameters (LightGBM): OrderedDict([('bagging_fraction', 0.5531550244558602), ('feature_fraction', 0.9396784095217918), ('lambda_l2', 1.1506549622849023), ('learning_rate', 0.2914997906774421), ('max_depth', 15), ('n_estimators', 492)])\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9396784095217918, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9396784095217918\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.1506549622849023, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1506549622849023\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5531550244558602, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5531550244558602\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9396784095217918, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9396784095217918\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.1506549622849023, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1506549622849023\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5531550244558602, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5531550244558602\n",
      "âœ… Selected Features (Mutual Information): ['V14', 'V17', 'V10', 'V12', 'V4', 'Amount', 'V11', 'V3', 'V16', 'V7', 'V2', 'V9', 'V27', 'V21', 'V18', 'V1', 'V6', 'V28', 'V5', 'V8']\n",
      "âœ… Selected Features(Overall): ['V14', 'V17', 'V4', 'Amount', 'V3', 'V9', 'V18', 'V1', 'V5', 'V8']\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9396784095217918, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9396784095217918\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.1506549622849023, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1506549622849023\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5531550244558602, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5531550244558602\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9396784095217918, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9396784095217918\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.1506549622849023, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1506549622849023\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5531550244558602, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5531550244558602\n",
      "--------------------------------------------------------------------------\n",
      "ðŸ“Š Model Accuracy Before Feature Selection: 0.9983\n",
      "ðŸ“Š Model Precision Before Feature Selection: 0.5000\n",
      "ðŸ“Š Model Recall Before Feature Selection: 0.8878\n",
      "ðŸ“Š Model F1 Score Before Feature Selection: 0.6397\n",
      "âœ… Notebook saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from skopt.space import Integer, Categorical\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from skopt.space import Real\n",
    "import dill\n",
    "\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "X = df.drop(columns=['Class'])\n",
    "y = df['Class']\n",
    "scaler = StandardScaler()\n",
    "X[['Time', 'Amount']] = scaler.fit_transform(X[['Time', 'Amount']])\n",
    "\n",
    "with open(\"scaler.pkl\", \"wb\") as file:\n",
    "    dill.dump(scaler, file)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "####Smote\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "###let's use a lightgbm algorithm here for now\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "lgbm.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "####Finding the best parameters\n",
    "param_space_lgbm = {\n",
    "    'n_estimators': Integer(50, 500),\n",
    "    'max_depth': Integer(3, 15),\n",
    "    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "    'lambda_l2': Real(1, 5),\n",
    "    'bagging_fraction': Real(0.5, 1.0),\n",
    "    'feature_fraction': Real(0.5, 1.0)\n",
    "}\n",
    "bayesian_search_lgbm = BayesSearchCV(lgbm, param_space_lgbm, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "bayesian_search_lgbm.fit(X_train_balanced, y_train_balanced)\n",
    "print(f\"Best Parameters (LightGBM): {bayesian_search_lgbm.best_params_}\")\n",
    "\n",
    "###Trian the model using the best features from above\n",
    "best_lgbm = LGBMClassifier(random_state=42, verbose=0, **bayesian_search_lgbm.best_params_)\n",
    "best_lgbm.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "#########Save the model\n",
    "import dill\n",
    "with open(\"fraudulent.pkl\", \"wb\") as file:\n",
    "    dill.dump(best_lgbm, file)\n",
    "\"\"\"\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "####using MI\n",
    "mi_scores = mutual_info_classif(X_train_balanced, y_train_balanced)\n",
    "mi_df = pd.DataFrame({'Feature': X_train_balanced.columns, 'MI Score': mi_scores})\n",
    "mi_df = mi_df.sort_values(by='MI Score', ascending=False)\n",
    "selected_features_mi = mi_df['Feature'].head(20).tolist()\n",
    "print(f\" Selected Features (Mutual Information): {selected_features_mi}\")\n",
    "################\n",
    "X_train_mi = X_train_balanced[selected_features_mi]\n",
    "rfe = RFE(estimator=lgbm, n_features_to_select=10)\n",
    "rfe.fit(X_train_mi, y_train_balanced)\n",
    "selected_features = X_train_mi.columns[rfe.support_].tolist()\n",
    "print(f\" Selected Features(Overall): {selected_features}\")\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#######Train the models based on the selected features\n",
    "X_train_selected = X_train_balanced[selected_features]  \n",
    "X_test_selected = X_test[selected_features]\n",
    "lgbm.fit(X_train_selected, y_train_balanced)\n",
    "y_pred = lgbm.predict(X_test_selected)\n",
    "\n",
    "#Formulate for diff measureing quantities\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "#Print out the scores after using feature selection\n",
    "print(f\" Model Accuracy After Feature Selection: {accuracy:.4f}\")\n",
    "print(f\" Model Precision After Feature Selection: {precision:.4f}\")\n",
    "print(f\" Model Recall After Feature Selection: {recall:.4f}\")\n",
    "print(f\" Model F1 Score After Feature Selection: {f1:.4f}\")\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "#without using feature selection and using all the features\n",
    "\n",
    "best_lgbm.fit(X_train_balanced, y_train_balanced)\n",
    "y_pred1 = lgbm.predict(X_test)\n",
    "\n",
    "#####\n",
    "#Formulate for diff measureing quantities\n",
    "accuracy1 = accuracy_score(y_test, y_pred1)\n",
    "precision1 = precision_score(y_test, y_pred1)\n",
    "recall1 = recall_score(y_test, y_pred1)\n",
    "f11 = f1_score(y_test, y_pred1)\n",
    "\n",
    "#Print out the scores before using feature selection\n",
    "print(f\"ðŸ“Š Model Accuracy Before Feature Selection: {accuracy1:.4f}\")\n",
    "print(f\"ðŸ“Š Model Precision Before Feature Selection: {precision1:.4f}\")\n",
    "print(f\"ðŸ“Š Model Recall Before Feature Selection: {recall1:.4f}\")\n",
    "print(f\"ðŸ“Š Model F1 Score Before Feature Selection: {f11:.4f}\")\n",
    "\n",
    "\n",
    "print(\"âœ… Notebook saved successfully!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a54f697e-efe9-43dd-b34f-126651f79d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Time (or leave blank if unknown):  0.8889696\n",
      "Enter V1 (or leave blank if unknown):  0.\n",
      "Enter V2 (or leave blank if unknown):  0\n",
      "Enter V3 (or leave blank if unknown):  0.26262\n",
      "Enter V4 (or leave blank if unknown):  0.6595495\n",
      "Enter V5 (or leave blank if unknown):  1.629595\n",
      "Enter V6 (or leave blank if unknown):  0.65495\n",
      "Enter V7 (or leave blank if unknown):  1.45899\n",
      "Enter V8 (or leave blank if unknown):  1.62198219\n",
      "Enter V9 (or leave blank if unknown):  1.9859898\n",
      "Enter V10 (or leave blank if unknown):  0.9898498\n",
      "Enter V11 (or leave blank if unknown):  0.89889\n",
      "Enter V12 (or leave blank if unknown):  1.569898\n",
      "Enter V13 (or leave blank if unknown):  0.56198489\n",
      "Enter V14 (or leave blank if unknown):  1.+189198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid input! Please enter a numeric value for V14.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter V14 (or leave blank if unknown):  0.11891\n",
      "Enter V15 (or leave blank if unknown):  2.519219851\n",
      "Enter V16 (or leave blank if unknown):  1.629982\n",
      "Enter V17 (or leave blank if unknown):  1.5215198\n",
      "Enter V18 (or leave blank if unknown):  0.5219298\n",
      "Enter V19 (or leave blank if unknown):  1.59589\n",
      "Enter V20 (or leave blank if unknown):  1.529889\n",
      "Enter V21 (or leave blank if unknown):  1.649898\n",
      "Enter V22 (or leave blank if unknown):  0.592982\n",
      "Enter V23 (or leave blank if unknown):  0.52665\n",
      "Enter V24 (or leave blank if unknown):  1.59298\n",
      "Enter V25 (or leave blank if unknown):  1.9559298\n",
      "Enter V26 (or leave blank if unknown):  0.9852985\n",
      "Enter V27 (or leave blank if unknown):  1.5219598\n",
      "Enter V28 (or leave blank if unknown):  0.98598\n",
      "Enter Amount (or leave blank if unknown):  138.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Input DataFrame (Before Handling Missing Values):\n",
      "      Time   V1   V2       V3       V4        V5       V6       V7        V8  \\\n",
      "0  0.88897  0.0  0.0  0.26262  0.65955  1.629595  0.65495  1.45899  1.621982   \n",
      "\n",
      "        V9  ...       V20       V21       V22      V23      V24      V25  \\\n",
      "0  1.98599  ...  1.529889  1.649898  0.592982  0.52665  1.59298  1.95593   \n",
      "\n",
      "        V26      V27      V28  Amount  \n",
      "0  0.985298  1.52196  0.98598  138.55  \n",
      "\n",
      "[1 rows x 30 columns]\n",
      "\n",
      "Missing values count: 0\n",
      "\n",
      "User DataFrame (After Handling Missing Values):\n",
      "      Time   V1   V2       V3       V4        V5       V6       V7        V8  \\\n",
      "0  0.88897  0.0  0.0  0.26262  0.65955  1.629595  0.65495  1.45899  1.621982   \n",
      "\n",
      "        V9  ...       V20       V21       V22      V23      V24      V25  \\\n",
      "0  1.98599  ...  1.529889  1.649898  0.592982  0.52665  1.59298  1.95593   \n",
      "\n",
      "        V26      V27      V28  Amount  \n",
      "0  0.985298  1.52196  0.98598  138.55  \n",
      "\n",
      "[1 rows x 30 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- V1\n- V10\n- V11\n- V12\n- V13\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(user_df_imputed)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Apply scaling to all features\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m user_df_imputed[feature_names] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(user_df_imputed[feature_names])\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Make a prediction\u001b[39;00m\n\u001b[0;32m     60\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(user_df_imputed)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:992\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m    989\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    991\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m--> 992\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    993\u001b[0m     X,\n\u001b[0;32m    994\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    995\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    996\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    997\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    998\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    999\u001b[0m )\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:548\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    490\u001b[0m ):\n\u001b[0;32m    491\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    551\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    552\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:481\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    477\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    479\u001b[0m     )\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- V1\n- V10\n- V11\n- V12\n- V13\n- ...\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the scaler\n",
    "with open(\"scaler.pkl\", \"rb\") as file:\n",
    "    scaler = dill.load(file)\n",
    "\n",
    "# Load the model\n",
    "with open(\"fraudulent.pkl\", \"rb\") as file:\n",
    "    model = dill.load(file)\n",
    "\n",
    "# Features list\n",
    "feature_names = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', \n",
    "                 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', \n",
    "                 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', \n",
    "                 'V28', 'Amount']\n",
    "\n",
    "# Accepting user inputs safely\n",
    "user_data = {}\n",
    "for feature in feature_names:\n",
    "    while True:\n",
    "        value = input(f\"Enter {feature} (or leave blank if unknown): \")\n",
    "        if value.strip() == \"\":\n",
    "            user_data[feature] = np.nan  # Handle missing values\n",
    "            break\n",
    "        try:\n",
    "            user_data[feature] = float(value)\n",
    "            break  # If input is valid, exit loop\n",
    "        except ValueError:\n",
    "            print(f\"Invalid input! Please enter a numeric value for {feature}.\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "user_df = pd.DataFrame([user_data])\n",
    "\n",
    "print(\"\\nUser Input DataFrame (Before Handling Missing Values):\")\n",
    "print(user_df)\n",
    "\n",
    "# Check how many missing values\n",
    "missing_count = user_df.isna().sum().sum()\n",
    "print(f\"\\nMissing values count: {missing_count}\")\n",
    "\n",
    "if missing_count > len(feature_names) // 2:\n",
    "    print(\"\\n Too many missing values! Prediction might not be accurate.\")\n",
    "\n",
    "# Fill missing values with median before scaling (to avoid errors)\n",
    "user_df_imputed = user_df.fillna(user_df.median())\n",
    "\n",
    "print(\"\\nUser DataFrame (After Handling Missing Values):\")\n",
    "print(user_df_imputed)\n",
    "\n",
    "# Apply scaling to all features\n",
    "user_df_scaled = scaler.transform(user_df_imputed[feature_names])\n",
    "\n",
    "# Make a prediction (ensure correct format for model)\n",
    "prediction = model.predict(pd.DataFrame(user_df_scaled, columns=feature_names))[0]\n",
    "\n",
    "if prediction == 1:\n",
    "    print(\"\\n ALERT: This transaction is likely FRAUDULENT!\")\n",
    "else:\n",
    "    print(\"\\n This transaction seems legitimate.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882145d1-3c3e-4e4d-a54b-e27e9f68e615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
